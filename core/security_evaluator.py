import os
import json
import numpy as np
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import streamlit as st

from .model_loader import ModelLoader
from .dataset_manager import DatasetManager
from .attack_manager import AttackManager
from .evaluation import (
    EvaluationConfig, DataProcessor, ARTEstimatorManager,
    AttackExecutor, MemoryManager, MetricsCalculator, ResultManager
)
from .evaluation.defense_evaluator import DefenseEvaluator  # æ·»åŠ è¿™è¡Œ
from .visualization import ChartGenerator
from .reporting import ReportGenerator

class SecurityEvaluator:
    """å®‰å…¨è¯„ä¼°å¼•æ“ - é‡æ„åçš„ä¸»åè°ƒå™¨"""
    
    def __init__(self):
        # æ ¸å¿ƒç»„ä»¶
        self.model_loader = ModelLoader()
        self.dataset_manager = DatasetManager()
        self.attack_manager = AttackManager()
        
        # è¯„ä¼°ç»„ä»¶
        self.data_processor = DataProcessor()
        self.estimator_manager = ARTEstimatorManager()
        self.memory_manager = MemoryManager()
        self.attack_executor = AttackExecutor(self.attack_manager, self.memory_manager)
        self.metrics_calculator = MetricsCalculator()
        self.result_manager = ResultManager()
        self.defense_evaluator = DefenseEvaluator()  # ç°åœ¨åº”è¯¥å¯ä»¥æ­£å¸¸å·¥ä½œ
        
        # å¯è§†åŒ–å’ŒæŠ¥å‘Šç»„ä»¶
        self.chart_generator = ChartGenerator()
        self.report_generator = ReportGenerator()
    
    def _ensure_directories(self):
        """ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        os.makedirs(self.results_dir, exist_ok=True)
        os.makedirs(self.reports_dir, exist_ok=True)
    
    def evaluate_model_robustness(self, model_info: Dict, dataset_info: Dict, 
                                attack_config: Dict, evaluation_params: Dict) -> Dict:
        """è¯„ä¼°æ¨¡å‹é²æ£’æ€§ - ç®€åŒ–çš„ä¸»æµç¨‹"""
        try:
            # 1. é…ç½®éªŒè¯
            eval_config = EvaluationConfig(
                sample_size=evaluation_params.get('sample_size', 1000),
                batch_size=attack_config.get('advanced_options', {}).get('batch_size', 32)
            )
            eval_config.validate()
            
            # 2. æ•°æ®å‡†å¤‡
            st.info("ğŸ”„ å‡†å¤‡æ•°æ®é›†...")
            x_data, y_data = self.data_processor.prepare_dataset(
                dataset_info, eval_config.sample_size
            )
            
            if x_data is None or y_data is None:
                return {"error": "æ•°æ®å‡†å¤‡å¤±è´¥"}
            
            # 3. åˆ›å»ºä¼°è®¡å™¨
            st.info("ğŸ¤– åˆ›å»ºæ¨¡å‹ä¼°è®¡å™¨...")
            input_shape = x_data.shape[1:]
            estimator = self.estimator_manager.create_estimator(model_info, input_shape)
            
            if estimator is None:
                return {"error": "æ¨¡å‹åŠ è½½å¤±è´¥"}
            
            # 4. éªŒè¯ä¼°è®¡å™¨
            if not self.estimator_manager.validate_estimator(estimator, x_data):
                return {"error": "æ¨¡å‹éªŒè¯å¤±è´¥"}
            
            # 5. è®¡ç®—åŸºçº¿æŒ‡æ ‡
            st.info("ğŸ“Š è®¡ç®—åŸºçº¿æŒ‡æ ‡...")
            baseline_metrics = self.metrics_calculator.calculate_baseline_metrics(
                estimator, x_data, y_data
            )
            
            if baseline_metrics is None:
                return {"error": "åŸºçº¿æŒ‡æ ‡è®¡ç®—å¤±è´¥"}
            
            if len(baseline_metrics['correctly_classified_indices']) == 0:
                return {"error": "æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®ä¸Šå‡†ç¡®ç‡ä¸º0ï¼Œæ— æ³•è¿›è¡Œæ”»å‡»è¯„ä¼°"}
            
            # 6. æ‰§è¡Œæ”»å‡»
            st.info("âš”ï¸ æ‰§è¡Œå¯¹æŠ—æ”»å‡»...")
            correctly_classified_indices = baseline_metrics['correctly_classified_indices']
            x_correct = x_data[correctly_classified_indices]
            y_correct = y_data[correctly_classified_indices] if y_data is not None else None
            
            adversarial_samples, attack_stats = self.attack_executor.execute_attack(
                estimator, x_correct, y_correct, attack_config, eval_config.batch_size
            )
            
            if adversarial_samples is None:
                return {"error": "æ”»å‡»æ‰§è¡Œå¤±è´¥"}
            
            # 7. è®¡ç®—æ”»å‡»æŒ‡æ ‡
            st.info("ğŸ“ˆ è®¡ç®—æ”»å‡»æ•ˆæœ...")
            attack_metrics = self.metrics_calculator.calculate_attack_metrics(
                estimator, x_data, y_data, adversarial_samples, baseline_metrics
            )
            
            if attack_metrics is None:
                return {"error": "æ”»å‡»æŒ‡æ ‡è®¡ç®—å¤±è´¥"}
            
            # 8. ç”Ÿæˆè¯„ä¼°ç»“æœ
            evaluation_result = self.result_manager.create_evaluation_result(
                model_info, dataset_info, attack_config, evaluation_params,
                baseline_metrics, attack_metrics, attack_stats
            )
            
            if evaluation_result is None:
                return {"error": "è¯„ä¼°ç»“æœç”Ÿæˆå¤±è´¥"}
            
            # 9. ä¿å­˜ç»“æœ
            if self.result_manager.save_evaluation_result(evaluation_result):
                # ä¿å­˜å¯¹æŠ—æ ·æœ¬ï¼ˆå¯é€‰ï¼‰
                if eval_config.save_adversarial_samples:
                    self.result_manager.save_adversarial_samples(
                        evaluation_result['evaluation_id'],
                        x_correct, adversarial_samples,
                        y_correct if y_correct is not None else baseline_metrics['y_true'][correctly_classified_indices]
                    )
                st.success("âœ… è¯„ä¼°å®Œæˆå¹¶ä¿å­˜æˆåŠŸ")
            
            return evaluation_result
            
        except Exception as e:
            st.error(f"å®‰å…¨è¯„ä¼°å¤±è´¥: {str(e)}")
            return {"error": str(e)}
    
    # ä¿ç•™å¿…è¦çš„æ¥å£æ–¹æ³•ï¼Œå§”æ‰˜ç»™ç›¸åº”çš„æ¨¡å—
    def get_evaluation_history(self, user_id: str = None) -> List[Dict]:
        """è·å–è¯„ä¼°å†å²"""
        return self.result_manager.get_evaluation_history(user_id)
    
    def generate_visualization(self, result: Dict) -> Dict:
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        return self.chart_generator.generate_charts(result)
    
    def generate_report(self, result: Dict, report_format: str = 'html') -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        return self.report_generator.generate_report(result, report_format)
    
    def get_storage_stats(self) -> Dict:
        """è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        return self.result_manager.get_storage_stats()
    
    def load_evaluation_result(self, evaluation_id: str) -> Optional[Dict]:
        """åŠ è½½è¯„ä¼°ç»“æœ"""
        return self.result_manager.load_evaluation_result(evaluation_id)
    
    def delete_evaluation(self, evaluation_id: str, user_id: str = None) -> bool:
        """åˆ é™¤è¯„ä¼°è®°å½•"""
        return self.result_manager.delete_evaluation(evaluation_id, user_id)
    
    # ä¸ºäº†å…¼å®¹æ€§ï¼Œä¿ç•™ä¸€äº›åŸæœ‰æ–¹æ³•çš„ç®€åŒ–ç‰ˆæœ¬
    def get_completed_evaluations(self) -> List[Dict]:
        """è·å–æ‰€æœ‰å·²å®Œæˆçš„è¯„ä¼°ï¼ˆç®¡ç†å‘˜ç”¨ï¼‰"""
        try:
            evaluations = []
            history = self.get_evaluation_history()
            
            for result in history:
                evaluation = {
                    'id': result.get('evaluation_id'),
                    'name': f"{result.get('model_info', {}).get('name', 'Unknown')} vs {result.get('attack_config', {}).get('algorithm_name', 'Unknown')}",
                    'type': 'security_evaluation',
                    'completed_at': result.get('timestamp'),
                    'results': result.get('results', {}),
                    'config': {
                        'model': result.get('model_info', {}),
                        'dataset': result.get('dataset_info', {}),
                        'attack_configs': [result.get('attack_config', {})]
                    }
                }
                evaluations.append(evaluation)
            
            return evaluations
            
        except Exception as e:
            st.error(f"è·å–å·²å®Œæˆè¯„ä¼°å¤±è´¥: {str(e)}")
            return []
    
    def get_user_completed_evaluations(self, user_id: str) -> List[Dict]:
        """è·å–æŒ‡å®šç”¨æˆ·çš„å·²å®Œæˆè¯„ä¼°"""
        try:
            evaluations = []
            history = self.get_evaluation_history(user_id)
            
            for result in history:
                evaluation = {
                    'id': result.get('evaluation_id'),
                    'name': f"{result.get('model_info', {}).get('name', 'Unknown')} vs {result.get('attack_config', {}).get('algorithm_name', 'Unknown')}",
                    'type': 'security_evaluation',
                    'completed_at': result.get('timestamp'),
                    'results': result.get('results', {}),
                    'config': {
                        'model': result.get('model_info', {}),
                        'dataset': result.get('dataset_info', {}),
                        'attack_configs': [result.get('attack_config', {})]
                    }
                }
                evaluations.append(evaluation)
            
            return evaluations
            
        except Exception as e:
            st.error(f"è·å–ç”¨æˆ·å·²å®Œæˆè¯„ä¼°å¤±è´¥: {str(e)}")
            return []
    
    def start_evaluation(self, evaluation_config: Dict) -> str:
        """å¯åŠ¨å®‰å…¨è¯„ä¼°"""
        try:
            attack_configs = evaluation_config.get('attack_configs', [])
            if not attack_configs:
                st.error("æ²¡æœ‰æ‰¾åˆ°æ”»å‡»é…ç½®")
                return None
            
            # å–ç¬¬ä¸€ä¸ªæ”»å‡»é…ç½®
            raw_attack_config = attack_configs[0]
            attack_config = raw_attack_config.get('config', raw_attack_config)
            
            evaluation_params = evaluation_config.get('parameters', {})
            
            result = self.evaluate_model_robustness(
                evaluation_config['model'],
                evaluation_config['dataset'],
                attack_config,
                evaluation_params
            )
            
            if 'error' in result:
                st.error(f"è¯„ä¼°å¤±è´¥: {result['error']}")
                return None
            
            return result.get('evaluation_id')
            
        except Exception as e:
            st.error(f"å¯åŠ¨è¯„ä¼°å¤±è´¥: {str(e)}")
            return None
    
    def get_all_evaluations(self) -> List[Dict]:
        """è·å–æ‰€æœ‰è¯„ä¼°è®°å½•ï¼ˆç®¡ç†å‘˜ç”¨ï¼‰"""
        return self.result_manager.get_all_evaluations()
    
    def get_user_evaluations(self, user_id: str) -> List[Dict]:
        """è·å–æŒ‡å®šç”¨æˆ·çš„è¯„ä¼°è®°å½•"""
        return self.result_manager.get_user_evaluations(user_id)
    
    def get_evaluation_stats(self) -> Dict:
        """è·å–è¯„ä¼°ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = {
                'total_evaluations': 0,
                'completed_evaluations': 0,
                'running_evaluations': 0,
                'evaluation_types': {},
                'user_activity': {}
            }
            
            # è·å–æ‰€æœ‰è¯„ä¼°å†å²
            history = self.get_evaluation_history()
            stats['total_evaluations'] = len(history)
            stats['completed_evaluations'] = len([r for r in history if 'results' in r])
            stats['running_evaluations'] = len([r for r in history if 'results' not in r])
            
            # ç»Ÿè®¡è¯„ä¼°ç±»å‹
            for result in history:
                attack_name = result.get('attack_config', {}).get('algorithm_name', 'Unknown')
                stats['evaluation_types'][attack_name] = stats['evaluation_types'].get(attack_name, 0) + 1
            
            # ç»Ÿè®¡ç”¨æˆ·æ´»åŠ¨ - ä¿®å¤æ•°æ®ç»“æ„
            for result in history:
                user_id = result.get('user_id', 'anonymous')
                if user_id not in stats['user_activity']:
                    stats['user_activity'][user_id] = {
                        'total': 0,
                        'completed': 0,
                        'running': 0,
                        'failed': 0
                    }
                
                stats['user_activity'][user_id]['total'] += 1
                
                # æ ¹æ®ç»“æœçŠ¶æ€åˆ†ç±»
                if 'results' in result:
                    if result.get('results', {}).get('error'):
                        stats['user_activity'][user_id]['failed'] += 1
                    else:
                        stats['user_activity'][user_id]['completed'] += 1
                else:
                    stats['user_activity'][user_id]['running'] += 1
            
            return stats
            
        except Exception as e:
            st.error(f"è·å–è¯„ä¼°ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {
                'total_evaluations': 0,
                'completed_evaluations': 0,
                'running_evaluations': 0,
                'evaluation_types': {},
                'user_activity': {}
            }
    
    def evaluate_model_with_defense(self, model_info, dataset_info, 
                                  attack_config, defense_config, evaluation_params):
        """å¸¦é˜²å¾¡çš„æ¨¡å‹è¯„ä¼°"""
        try:
            # 1. æ‰§è¡ŒåŸºç¡€æ”»å‡»è¯„ä¼°
            attack_result = self.evaluate_model_robustness(
                model_info, dataset_info, attack_config, evaluation_params
            )
            
            if 'error' in attack_result:
                return attack_result
            
            # 2. æ‰§è¡Œé˜²å¾¡è¯„ä¼°
            defense_result = self.defense_evaluator.evaluate_defense(
                attack_result['model'], 
                attack_result['clean_data'],
                attack_result['labels'],
                attack_result['adversarial_samples'],
                defense_config
            )
            
            # 3. åˆå¹¶ç»“æœ
            combined_result = {
                **attack_result,
                'defense_metrics': defense_result,
                'evaluation_type': 'attack_and_defense'
            }
            
            return combined_result
            
        except Exception as e:
            return {"error": f"å¸¦é˜²å¾¡çš„è¯„ä¼°å¤±è´¥: {str(e)}"}