import os
import json
import numpy as np
import pandas as pd
import torch
import tensorflow as tf
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import streamlit as st
from art.estimators.classification import (
    PyTorchClassifier, TensorFlowV2Classifier, KerasClassifier
)
from art.utils import load_mnist, load_cifar10
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

from .model_loader import ModelLoader
from .dataset_manager import DatasetManager
from .attack_manager import AttackManager

class SecurityEvaluator:
    """å®‰å…¨è¯„ä¼°å¼•æ“"""
    
    def __init__(self):
        self.model_loader = ModelLoader()
        self.dataset_manager = DatasetManager()
        self.attack_manager = AttackManager()
        self.results_dir = "data/evaluation_results"
        self.reports_dir = "data/reports"
        self._ensure_directories()
    
    def _ensure_directories(self):
        """ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        os.makedirs(self.results_dir, exist_ok=True)
        os.makedirs(self.reports_dir, exist_ok=True)
    
    def create_art_estimator(self, model_info: Dict, input_shape: Tuple) -> Optional[Any]:
        """åˆ›å»ºARTä¼°è®¡å™¨"""
        try:
            model_path = model_info['file_path']
            model_type = model_info['model_type']
            
            if model_type == 'pytorch':
                # åŠ è½½PyTorchæ¨¡å‹
                model = torch.load(model_path, map_location='cpu')
                model.eval()
                
                # åˆ›å»ºæŸå¤±å‡½æ•°
                criterion = torch.nn.CrossEntropyLoss()
                
                # åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆç”¨äºå¯¹æŠ—è®­ç»ƒï¼Œè¿™é‡Œä½¿ç”¨é»˜è®¤å‚æ•°ï¼‰
                optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
                
                # åˆ›å»ºARTåˆ†ç±»å™¨
                estimator = PyTorchClassifier(
                    model=model,
                    loss=criterion,
                    optimizer=optimizer,
                    input_shape=input_shape,
                    nb_classes=model_info.get('num_classes', 10)
                )
                
            elif model_type in ['tensorflow', 'keras']:
                # åŠ è½½TensorFlow/Kerasæ¨¡å‹
                model = tf.keras.models.load_model(model_path)
                
                # åˆ›å»ºARTåˆ†ç±»å™¨
                estimator = TensorFlowV2Classifier(
                    model=model,
                    nb_classes=model_info.get('num_classes', 10),
                    input_shape=input_shape
                )
                
            else:
                st.error(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
                return None
            
            return estimator
            
        except Exception as e:
            st.error(f"åˆ›å»ºARTä¼°è®¡å™¨å¤±è´¥: {str(e)}")
            return None
    
    def prepare_dataset(self, dataset_info: Dict, sample_size: int = None) -> Tuple[np.ndarray, np.ndarray]:
        """å‡†å¤‡æ•°æ®é›†"""
        try:
            if dataset_info['dataset_type'] == 'builtin':
                # å†…ç½®æ•°æ®é›†
                dataset_name = dataset_info['name']
                
                if dataset_name == 'MNIST':
                    (x_train, y_train), (x_test, y_test), _, _ = load_mnist()
                    x_data, y_data = x_test, y_test
                elif dataset_name == 'CIFAR-10':
                    (x_train, y_train), (x_test, y_test), _, _ = load_cifar10()
                    x_data, y_data = x_test, y_test
                else:
                    st.error(f"ä¸æ”¯æŒçš„å†…ç½®æ•°æ®é›†: {dataset_name}")
                    return None, None
                    
            else:
                # ç”¨æˆ·ä¸Šä¼ çš„æ•°æ®é›†
                dataset_path = dataset_info['file_path']
                data_format = dataset_info['data_format']
                
                if data_format == 'numpy':
                    data = np.load(dataset_path, allow_pickle=True)
                    if isinstance(data, dict):
                        x_data = data['x'] if 'x' in data else data['data']
                        y_data = data['y'] if 'y' in data else data['labels']
                    else:
                        # å‡è®¾æ˜¯ç‰¹å¾æ•°æ®ï¼Œæ ‡ç­¾éœ€è¦å•ç‹¬åŠ è½½
                        x_data = data
                        y_data = None
                        
                elif data_format == 'csv':
                    df = pd.read_csv(dataset_path)
                    # å‡è®¾æœ€åä¸€åˆ—æ˜¯æ ‡ç­¾
                    x_data = df.iloc[:, :-1].values
                    y_data = df.iloc[:, -1].values
                    
                else:
                    st.error(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {data_format}")
                    return None, None
            
            # é‡‡æ ·
            if sample_size and len(x_data) > sample_size:
                indices = np.random.choice(len(x_data), sample_size, replace=False)
                x_data = x_data[indices]
                if y_data is not None:
                    y_data = y_data[indices]
            
            # æ•°æ®é¢„å¤„ç†
            if x_data.dtype != np.float32:
                x_data = x_data.astype(np.float32)
            
            # å½’ä¸€åŒ–åˆ°[0,1]
            if x_data.max() > 1.0:
                x_data = x_data / 255.0
            
            return x_data, y_data
            
        except Exception as e:
            st.error(f"å‡†å¤‡æ•°æ®é›†å¤±è´¥: {str(e)}")
            return None, None
    
    def evaluate_model_robustness(self, model_info: Dict, dataset_info: Dict, 
                                attack_config: Dict, evaluation_params: Dict) -> Dict:
        """è¯„ä¼°æ¨¡å‹é²æ£’æ€§"""
        try:
            # å‡†å¤‡æ•°æ®
            x_data, y_data = self.prepare_dataset(
                dataset_info, 
                evaluation_params.get('sample_size', 1000)
            )
            
            if x_data is None or y_data is None:
                return {"error": "æ•°æ®å‡†å¤‡å¤±è´¥"}
            
            # åˆ›å»ºARTä¼°è®¡å™¨
            input_shape = x_data.shape[1:]
            estimator = self.create_art_estimator(model_info, input_shape)
            
            if estimator is None:
                return {"error": "æ¨¡å‹åŠ è½½å¤±è´¥"}
            
            # è·å–åŸå§‹é¢„æµ‹
            st.info("ğŸ” è·å–åŸå§‹æ¨¡å‹é¢„æµ‹...")
            original_predictions = estimator.predict(x_data)
            original_accuracy = accuracy_score(y_data, np.argmax(original_predictions, axis=1))
            
            # åˆ›å»ºæ”»å‡»å®ä¾‹
            st.info("âš”ï¸ åˆ›å»ºæ”»å‡»å®ä¾‹...")
            attack_instance = self.attack_manager.create_attack_instance(
                attack_config['algorithm'],
                attack_config['params'],
                estimator
            )
            
            if attack_instance is None:
                return {"error": "æ”»å‡»å®ä¾‹åˆ›å»ºå¤±è´¥"}
            
            # ç”Ÿæˆå¯¹æŠ—æ ·æœ¬
            st.info("ğŸ¯ ç”Ÿæˆå¯¹æŠ—æ ·æœ¬...")
            batch_size = attack_config['advanced_options'].get('batch_size', 32)
            
            adversarial_samples = []
            for i in range(0, len(x_data), batch_size):
                batch_x = x_data[i:i+batch_size]
                batch_y = y_data[i:i+batch_size] if y_data is not None else None
                
                # ç”Ÿæˆå¯¹æŠ—æ ·æœ¬
                if attack_config.get('targeted', False) and batch_y is not None:
                    # ç›®æ ‡æ”»å‡»
                    adv_batch = attack_instance.generate(x=batch_x, y=batch_y)
                else:
                    # éç›®æ ‡æ”»å‡»
                    adv_batch = attack_instance.generate(x=batch_x)
                
                adversarial_samples.append(adv_batch)
                
                # æ›´æ–°è¿›åº¦
                progress = min((i + batch_size) / len(x_data), 1.0)
                st.progress(progress)
            
            adversarial_samples = np.concatenate(adversarial_samples, axis=0)
            
            # è¯„ä¼°å¯¹æŠ—æ ·æœ¬
            st.info("ğŸ“Š è¯„ä¼°å¯¹æŠ—æ ·æœ¬æ•ˆæœ...")
            adversarial_predictions = estimator.predict(adversarial_samples)
            adversarial_accuracy = accuracy_score(y_data, np.argmax(adversarial_predictions, axis=1))
            
            # è®¡ç®—æ”»å‡»æˆåŠŸç‡
            attack_success_rate = 1.0 - adversarial_accuracy
            
            # è®¡ç®—æ‰°åŠ¨ç»Ÿè®¡
            perturbations = adversarial_samples - x_data
            l0_norm = np.mean(np.sum(perturbations != 0, axis=tuple(range(1, len(perturbations.shape)))))
            l2_norm = np.mean(np.sqrt(np.sum(perturbations ** 2, axis=tuple(range(1, len(perturbations.shape))))))
            linf_norm = np.mean(np.max(np.abs(perturbations), axis=tuple(range(1, len(perturbations.shape)))))
            
            # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
            evaluation_result = {
                "evaluation_id": f"eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                "timestamp": datetime.now().isoformat(),
                "model_info": model_info,
                "dataset_info": dataset_info,
                "attack_config": attack_config,
                "evaluation_params": evaluation_params,
                "results": {
                    "original_accuracy": float(original_accuracy),
                    "adversarial_accuracy": float(adversarial_accuracy),
                    "attack_success_rate": float(attack_success_rate),
                    "robustness_score": float(1.0 - attack_success_rate),
                    "perturbation_stats": {
                        "l0_norm": float(l0_norm),
                        "l2_norm": float(l2_norm),
                        "linf_norm": float(linf_norm)
                    },
                    "sample_count": len(x_data),
                    "successful_attacks": int(len(x_data) * attack_success_rate)
                },
                "detailed_metrics": {
                    "original_classification_report": classification_report(
                        y_data, np.argmax(original_predictions, axis=1), output_dict=True
                    ),
                    "adversarial_classification_report": classification_report(
                        y_data, np.argmax(adversarial_predictions, axis=1), output_dict=True
                    )
                }
            }
            
            # ä¿å­˜ç»“æœ
            if evaluation_params.get('save_results', True):
                self.save_evaluation_result(evaluation_result)
            
            # ä¿å­˜å¯¹æŠ—æ ·æœ¬ï¼ˆå¯é€‰ï¼‰
            if attack_config['advanced_options'].get('save_adversarial', False):
                self.save_adversarial_samples(
                    evaluation_result['evaluation_id'],
                    x_data, adversarial_samples, y_data
                )
            
            return evaluation_result
            
        except Exception as e:
            st.error(f"å®‰å…¨è¯„ä¼°å¤±è´¥: {str(e)}")
            return {"error": str(e)}
    
    def save_evaluation_result(self, result: Dict) -> bool:
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        try:
            result_file = os.path.join(
                self.results_dir, 
                f"{result['evaluation_id']}.json"
            )
            
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            return True
        except Exception as e:
            st.error(f"ä¿å­˜è¯„ä¼°ç»“æœå¤±è´¥: {str(e)}")
            return False
    
    def save_adversarial_samples(self, evaluation_id: str, original: np.ndarray, 
                               adversarial: np.ndarray, labels: np.ndarray) -> bool:
        """ä¿å­˜å¯¹æŠ—æ ·æœ¬"""
        try:
            samples_file = os.path.join(
                self.results_dir, 
                f"{evaluation_id}_samples.npz"
            )
            
            np.savez_compressed(
                samples_file,
                original=original,
                adversarial=adversarial,
                labels=labels
            )
            
            return True
        except Exception as e:
            st.error(f"ä¿å­˜å¯¹æŠ—æ ·æœ¬å¤±è´¥: {str(e)}")
            return False
    
    def load_evaluation_result(self, evaluation_id: str) -> Optional[Dict]:
        """åŠ è½½è¯„ä¼°ç»“æœ"""
        try:
            result_file = os.path.join(self.results_dir, f"{evaluation_id}.json")
            
            if os.path.exists(result_file):
                with open(result_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            
            return None
        except Exception as e:
            st.error(f"åŠ è½½è¯„ä¼°ç»“æœå¤±è´¥: {str(e)}")
            return None
    
    def get_evaluation_history(self, user_id: str = None) -> List[Dict]:
        """è·å–è¯„ä¼°å†å²"""
        results = []
        try:
            for filename in os.listdir(self.results_dir):
                if filename.endswith('.json'):
                    result_file = os.path.join(self.results_dir, filename)
                    with open(result_file, 'r', encoding='utf-8') as f:
                        result = json.load(f)
                        
                        # å¦‚æœæŒ‡å®šäº†ç”¨æˆ·IDï¼Œåªè¿”å›è¯¥ç”¨æˆ·çš„ç»“æœ
                        if user_id is None or result.get('model_info', {}).get('user_id') == user_id:
                            results.append(result)
        
        except Exception as e:
            st.error(f"è·å–è¯„ä¼°å†å²å¤±è´¥: {str(e)}")
        
        return sorted(results, key=lambda x: x.get('timestamp', ''), reverse=True)
    
    def generate_visualization(self, result: Dict) -> Dict:
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        try:
            visualizations = {}
            
            # 1. å‡†ç¡®ç‡å¯¹æ¯”å›¾
            accuracy_fig = go.Figure(data=[
                go.Bar(
                    x=['åŸå§‹æ¨¡å‹', 'å¯¹æŠ—æ”»å‡»å'],
                    y=[result['results']['original_accuracy'], 
                       result['results']['adversarial_accuracy']],
                    marker_color=['#2E86AB', '#A23B72'],
                    text=[f"{result['results']['original_accuracy']:.3f}", 
                          f"{result['results']['adversarial_accuracy']:.3f}"],
                    textposition='auto'
                )
            ])
            
            accuracy_fig.update_layout(
                title='æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”',
                yaxis_title='å‡†ç¡®ç‡',
                showlegend=False
            )
            
            visualizations['accuracy_comparison'] = accuracy_fig
            
            # 2. é²æ£’æ€§æŒ‡æ ‡é›·è¾¾å›¾
            metrics = {
                'åŸå§‹å‡†ç¡®ç‡': result['results']['original_accuracy'],
                'é²æ£’æ€§å¾—åˆ†': result['results']['robustness_score'],
                'æŠ—æ”»å‡»èƒ½åŠ›': 1.0 - result['results']['attack_success_rate'],
                'æ‰°åŠ¨æ•æ„Ÿæ€§': 1.0 - result['results']['perturbation_stats']['linf_norm']
            }
            
            radar_fig = go.Figure()
            
            radar_fig.add_trace(go.Scatterpolar(
                r=list(metrics.values()),
                theta=list(metrics.keys()),
                fill='toself',
                name='æ¨¡å‹æ€§èƒ½'
            ))
            
            radar_fig.update_layout(
                polar=dict(
                    radialaxis=dict(
                        visible=True,
                        range=[0, 1]
                    )
                ),
                title='æ¨¡å‹é²æ£’æ€§é›·è¾¾å›¾'
            )
            
            visualizations['robustness_radar'] = radar_fig
            
            # 3. æ‰°åŠ¨ç»Ÿè®¡å›¾
            perturbation_fig = go.Figure(data=[
                go.Bar(
                    x=['L0èŒƒæ•°', 'L2èŒƒæ•°', 'LâˆèŒƒæ•°'],
                    y=[result['results']['perturbation_stats']['l0_norm'],
                       result['results']['perturbation_stats']['l2_norm'],
                       result['results']['perturbation_stats']['linf_norm']],
                    marker_color=['#F18F01', '#C73E1D', '#592E83']
                )
            ])
            
            perturbation_fig.update_layout(
                title='æ‰°åŠ¨ç»Ÿè®¡',
                yaxis_title='æ‰°åŠ¨å¤§å°'
            )
            
            visualizations['perturbation_stats'] = perturbation_fig
            
            return visualizations
            
        except Exception as e:
            st.error(f"ç”Ÿæˆå¯è§†åŒ–å¤±è´¥: {str(e)}")
            return {}
    
    def generate_report(self, result: Dict, report_format: str = 'html') -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        try:
            report_id = f"report_{result['evaluation_id']}"
            
            if report_format == 'html':
                report_content = self._generate_html_report(result)
                report_file = os.path.join(self.reports_dir, f"{report_id}.html")
            elif report_format == 'pdf':
                report_content = self._generate_pdf_report(result)
                report_file = os.path.join(self.reports_dir, f"{report_id}.pdf")
            else:
                report_content = self._generate_text_report(result)
                report_file = os.path.join(self.reports_dir, f"{report_id}.txt")
            
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            return report_file
            
        except Exception as e:
            st.error(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {str(e)}")
            return None
    
    def _generate_html_report(self, result: Dict) -> str:
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        html_template = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>AIæ¨¡å‹å®‰å…¨è¯„ä¼°æŠ¥å‘Š</title>
            <meta charset="utf-8">
            <style>
                body {{ font-family: Arial, sans-serif; margin: 40px; }}
                .header {{ text-align: center; margin-bottom: 30px; }}
                .section {{ margin-bottom: 25px; }}
                .metric {{ background: #f5f5f5; padding: 10px; margin: 5px 0; border-radius: 5px; }}
                .success {{ color: #28a745; }}
                .warning {{ color: #ffc107; }}
                .danger {{ color: #dc3545; }}
                table {{ width: 100%; border-collapse: collapse; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>AIæ¨¡å‹å®‰å…¨è¯„ä¼°æŠ¥å‘Š</h1>
                <p>è¯„ä¼°ID: {result['evaluation_id']}</p>
                <p>è¯„ä¼°æ—¶é—´: {result['timestamp']}</p>
            </div>
            
            <div class="section">
                <h2>è¯„ä¼°æ¦‚è¦</h2>
                <div class="metric">æ¨¡å‹åç§°: {result['model_info']['name']}</div>
                <div class="metric">æ•°æ®é›†: {result['dataset_info']['name']}</div>
                <div class="metric">æ”»å‡»ç®—æ³•: {result['attack_config']['algorithm_name']}</div>
                <div class="metric">æ ·æœ¬æ•°é‡: {result['results']['sample_count']}</div>
            </div>
            
            <div class="section">
                <h2>è¯„ä¼°ç»“æœ</h2>
                <div class="metric">åŸå§‹å‡†ç¡®ç‡: <span class="success">{result['results']['original_accuracy']:.3f}</span></div>
                <div class="metric">æ”»å‡»åå‡†ç¡®ç‡: <span class="danger">{result['results']['adversarial_accuracy']:.3f}</span></div>
                <div class="metric">æ”»å‡»æˆåŠŸç‡: <span class="warning">{result['results']['attack_success_rate']:.3f}</span></div>
                <div class="metric">é²æ£’æ€§å¾—åˆ†: <span class="{'success' if result['results']['robustness_score'] > 0.7 else 'warning' if result['results']['robustness_score'] > 0.3 else 'danger'}">{result['results']['robustness_score']:.3f}</span></div>
            </div>
            
            <div class="section">
                <h2>æ‰°åŠ¨åˆ†æ</h2>
                <div class="metric">L0èŒƒæ•°: {result['results']['perturbation_stats']['l0_norm']:.6f}</div>
                <div class="metric">L2èŒƒæ•°: {result['results']['perturbation_stats']['l2_norm']:.6f}</div>
                <div class="metric">LâˆèŒƒæ•°: {result['results']['perturbation_stats']['linf_norm']:.6f}</div>
            </div>
            
            <div class="section">
                <h2>å®‰å…¨å»ºè®®</h2>
                <ul>
                    {'<li>æ¨¡å‹å…·æœ‰è‰¯å¥½çš„é²æ£’æ€§ï¼Œå»ºè®®ç»§ç»­ä¿æŒå½“å‰çš„å®‰å…¨æªæ–½ã€‚</li>' if result['results']['robustness_score'] > 0.7 else ''}
                    {'<li>æ¨¡å‹é²æ£’æ€§ä¸­ç­‰ï¼Œå»ºè®®è€ƒè™‘å¯¹æŠ—è®­ç»ƒæˆ–é˜²å¾¡æœºåˆ¶ã€‚</li>' if 0.3 <= result['results']['robustness_score'] <= 0.7 else ''}
                    {'<li>æ¨¡å‹é²æ£’æ€§è¾ƒå·®ï¼Œå¼ºçƒˆå»ºè®®å®æ–½å¯¹æŠ—è®­ç»ƒå’Œå¤šå±‚é˜²å¾¡ç­–ç•¥ã€‚</li>' if result['results']['robustness_score'] < 0.3 else ''}
                    <li>å®šæœŸè¿›è¡Œå®‰å…¨è¯„ä¼°ï¼Œç›‘æ§æ¨¡å‹åœ¨æ–°æ”»å‡»ä¸‹çš„è¡¨ç°ã€‚</li>
                    <li>è€ƒè™‘éƒ¨ç½²è¾“å…¥éªŒè¯å’Œå¼‚å¸¸æ£€æµ‹æœºåˆ¶ã€‚</li>
                </ul>
            </div>
        </body>
        </html>
        """
        
        return html_template
    
    def _generate_text_report(self, result: Dict) -> str:
        """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š"""
        report = f"""
===========================================
        AIæ¨¡å‹å®‰å…¨è¯„ä¼°æŠ¥å‘Š
===========================================

è¯„ä¼°ID: {result['evaluation_id']}
è¯„ä¼°æ—¶é—´: {result['timestamp']}

-------------------------------------------
è¯„ä¼°æ¦‚è¦
-------------------------------------------
æ¨¡å‹åç§°: {result['model_info']['name']}
æ¨¡å‹ç±»å‹: {result['model_info']['model_type']}
æ•°æ®é›†: {result['dataset_info']['name']}
æ”»å‡»ç®—æ³•: {result['attack_config']['algorithm_name']}
æ ·æœ¬æ•°é‡: {result['results']['sample_count']}

-------------------------------------------
è¯„ä¼°ç»“æœ
-------------------------------------------
åŸå§‹å‡†ç¡®ç‡: {result['results']['original_accuracy']:.3f}
æ”»å‡»åå‡†ç¡®ç‡: {result['results']['adversarial_accuracy']:.3f}
æ”»å‡»æˆåŠŸç‡: {result['results']['attack_success_rate']:.3f}
é²æ£’æ€§å¾—åˆ†: {result['results']['robustness_score']:.3f}
æˆåŠŸæ”»å‡»æ•°: {result['results']['successful_attacks']}

-------------------------------------------
æ‰°åŠ¨åˆ†æ
-------------------------------------------
L0èŒƒæ•° (ç¨€ç–æ€§): {result['results']['perturbation_stats']['l0_norm']:.6f}
L2èŒƒæ•° (æ¬§å‡ é‡Œå¾—è·ç¦»): {result['results']['perturbation_stats']['l2_norm']:.6f}
LâˆèŒƒæ•° (æœ€å¤§æ‰°åŠ¨): {result['results']['perturbation_stats']['linf_norm']:.6f}

-------------------------------------------
å®‰å…¨è¯„çº§
-------------------------------------------
{'ğŸŸ¢ ä¼˜ç§€ - æ¨¡å‹å…·æœ‰å¾ˆå¼ºçš„é²æ£’æ€§' if result['results']['robustness_score'] > 0.8 else ''}
{'ğŸŸ¡ è‰¯å¥½ - æ¨¡å‹å…·æœ‰è¾ƒå¥½çš„é²æ£’æ€§' if 0.6 <= result['results']['robustness_score'] <= 0.8 else ''}
{'ğŸŸ  ä¸€èˆ¬ - æ¨¡å‹é²æ£’æ€§ä¸­ç­‰ï¼Œéœ€è¦æ”¹è¿›' if 0.3 <= result['results']['robustness_score'] < 0.6 else ''}
{'ğŸ”´ è¾ƒå·® - æ¨¡å‹é²æ£’æ€§ä¸è¶³ï¼Œå­˜åœ¨å®‰å…¨é£é™©' if result['results']['robustness_score'] < 0.3 else ''}

-------------------------------------------
æ”¹è¿›å»ºè®®
-------------------------------------------
1. å®æ–½å¯¹æŠ—è®­ç»ƒæé«˜æ¨¡å‹é²æ£’æ€§
2. éƒ¨ç½²è¾“å…¥é¢„å¤„ç†å’ŒéªŒè¯æœºåˆ¶
3. è€ƒè™‘é›†æˆå¤šä¸ªé˜²å¾¡ç­–ç•¥
4. å®šæœŸè¿›è¡Œå®‰å…¨è¯„ä¼°å’Œç›‘æ§
5. å»ºç«‹æ”»å‡»æ£€æµ‹å’Œå“åº”æœºåˆ¶

===========================================
        """
        
        return report
    
    def get_storage_stats(self) -> Dict:
        """è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_evaluations": 0,
            "total_size": 0,
            "by_user": {},
            "by_model_type": {},
            "recent_evaluations": 0  # æœ€è¿‘30å¤©çš„è¯„ä¼°æ•°é‡
        }
        
        try:
            from datetime import datetime, timedelta
            thirty_days_ago = datetime.now() - timedelta(days=30)
            
            for filename in os.listdir(self.results_dir):
                if filename.endswith('.json'):
                    file_path = os.path.join(self.results_dir, filename)
                    file_size = os.path.getsize(file_path)
                    
                    stats["total_evaluations"] += 1
                    stats["total_size"] += file_size
                    
                    # è¯»å–è¯„ä¼°ç»“æœè·å–æ›´å¤šç»Ÿè®¡ä¿¡æ¯
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            result = json.load(f)
                            
                            # æŒ‰ç”¨æˆ·ç»Ÿè®¡
                            user_id = result.get('model_info', {}).get('uploader', 'Unknown')
                            if user_id not in stats["by_user"]:
                                stats["by_user"][user_id] = {'count': 0, 'size': 0}
                            stats["by_user"][user_id]['count'] += 1
                            stats["by_user"][user_id]['size'] += file_size
                            
                            # æŒ‰æ¨¡å‹ç±»å‹ç»Ÿè®¡
                            model_type = result.get('model_info', {}).get('model_type', 'Unknown')
                            if model_type not in stats["by_model_type"]:
                                stats["by_model_type"][model_type] = {'count': 0, 'size': 0}
                            stats["by_model_type"][model_type]['count'] += 1
                            stats["by_model_type"][model_type]['size'] += file_size
                            
                            # ç»Ÿè®¡æœ€è¿‘30å¤©çš„è¯„ä¼°
                            timestamp_str = result.get('timestamp', '')
                            if timestamp_str:
                                try:
                                    eval_time = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
                                    if eval_time >= thirty_days_ago:
                                        stats["recent_evaluations"] += 1
                                except ValueError:
                                    pass  # å¿½ç•¥æ—¶é—´æ ¼å¼é”™è¯¯
                                    
                    except (json.JSONDecodeError, KeyError):
                        continue  # å¿½ç•¥æŸåçš„æ–‡ä»¶
                        
        except Exception as e:
            st.error(f"è·å–å­˜å‚¨ç»Ÿè®¡å¤±è´¥: {str(e)}")
            
        return stats
    
    def get_completed_evaluations(self) -> List[Dict]:
        """è·å–æ‰€æœ‰å·²å®Œæˆçš„è¯„ä¼°ï¼ˆç®¡ç†å‘˜ç”¨ï¼‰"""
        try:
            evaluations = []
            for filename in os.listdir(self.results_dir):
                if filename.endswith('.json'):
                    result_file = os.path.join(self.results_dir, filename)
                    with open(result_file, 'r', encoding='utf-8') as f:
                        result = json.load(f)
                        
                        # è½¬æ¢ä¸ºé¡µé¢æœŸæœ›çš„æ ¼å¼
                        evaluation = {
                            'id': result.get('evaluation_id'),
                            'name': f"{result.get('model_info', {}).get('name', 'Unknown')} vs {result.get('attack_config', {}).get('algorithm_name', 'Unknown')}",
                            'type': 'security_evaluation',
                            'completed_at': result.get('timestamp'),
                            'results': result.get('results', {}),
                            'config': {
                                'model': result.get('model_info', {}),
                                'dataset': result.get('dataset_info', {}),
                                'attack_configs': [result.get('attack_config', {})]
                            }
                        }
                        evaluations.append(evaluation)
            
            return sorted(evaluations, key=lambda x: x.get('completed_at', ''), reverse=True)
            
        except Exception as e:
            st.error(f"è·å–å·²å®Œæˆè¯„ä¼°å¤±è´¥: {str(e)}")
            return []
    
    def get_user_completed_evaluations(self, user_id: str) -> List[Dict]:
        """è·å–æŒ‡å®šç”¨æˆ·çš„å·²å®Œæˆè¯„ä¼°"""
        try:
            evaluations = []
            for filename in os.listdir(self.results_dir):
                if filename.endswith('.json'):
                    result_file = os.path.join(self.results_dir, filename)
                    with open(result_file, 'r', encoding='utf-8') as f:
                        result = json.load(f)
                        
                        # æ£€æŸ¥æ˜¯å¦å±äºæŒ‡å®šç”¨æˆ·
                        if result.get('model_info', {}).get('uploader') == user_id:
                            # è½¬æ¢ä¸ºé¡µé¢æœŸæœ›çš„æ ¼å¼
                            evaluation = {
                                'id': result.get('evaluation_id'),
                                'name': f"{result.get('model_info', {}).get('name', 'Unknown')} vs {result.get('attack_config', {}).get('algorithm_name', 'Unknown')}",
                                'type': 'security_evaluation',
                                'completed_at': result.get('timestamp'),
                                'results': result.get('results', {}),
                                'config': {
                                    'model': result.get('model_info', {}),
                                    'dataset': result.get('dataset_info', {}),
                                    'attack_configs': [result.get('attack_config', {})]
                                }
                            }
                            evaluations.append(evaluation)
            
            return sorted(evaluations, key=lambda x: x.get('completed_at', ''), reverse=True)
            
        except Exception as e:
            st.error(f"è·å–ç”¨æˆ·å·²å®Œæˆè¯„ä¼°å¤±è´¥: {str(e)}")
            return []